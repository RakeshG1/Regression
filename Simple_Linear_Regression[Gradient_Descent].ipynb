{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `About Gradient & Derivatives`\n",
    "- Source Link : **`https://medium.com/intro-to-artificial-intelligence/gradient-descent-algorithm-explained-with-linear-regression-example-ff6b5491fdb9`**\n",
    "- Gradient descent algorithm is an optimisation algorithm that uses to find the optimal value of parameters that minimises loss function.\n",
    "- For instance, the algorithm iteratively adjusts the parameters such as weights and biases of the neural network to find the optimal parameters that minimise the loss function. \n",
    "- `A partial derivative of a function of multi variables is its derivative with respect to one of those variables, with the others held constant`. \n",
    "  - ![](readme_images/Partial_Derivative_Func_w.r.t_x.png)\n",
    "  - ![](readme_images/Partial_Derivative_Func_w.r.t_y.png)\n",
    "  - ![](readme_images/Gradient_Derivatives.png)\n",
    "- It gives the `rate of change of a function in the direction of a variable`.\n",
    "- It can determine how changes in the direction of a variable influence the function\n",
    "- Consider the multivariable function f(x, y) = xy²+x³. We can find partial derivatives of the function that is derivatives of function wrt to each variable x and y(a bit of calculus knowledge required to compute the partial derivatives).\n",
    "- Intuitively, a derivative of a function is the slope of the tangent line that gives a rate of change in a given point as shown above. If the function is higher-dimensional we have to find the partial derivatives to find the rate of change of the function at a given point. \n",
    "- ` In higher dimension, a gradient is a vector that contains partial derivatives to determine the rate of change. In a higher dimensional function, We can consider gradient as the slope`.\n",
    "- `In a lower-dimensional function, the gradient is a slope of the tangent line that determines the rate of change at a given point.`\n",
    "  - `Slope measurse both the direction and the steepness of the line`\n",
    "- The `gradient` gives the direction of the maximum change and the magnitude indicates the `maximum rate of change`. The gradient always points in the direction of the steepest increase in the objective function.   \n",
    "- If we update variables or parameters of some cost function in the direction of the negative gradient in an iterative manner to reach the minimum of some cost function is called `gradient descent algorithm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Gradient Descent as per - Linear Regression`\n",
    "- 1. Initialise the coefficients m and b with random values\n",
    "  - For example m = 1 and b =2, i.e a line equation is y = mx+b, then y = 1*x+2\n",
    "- 2. Calculate / compute gardient\n",
    "  - We use the `Sum of Squared Errors (SSE) as our loss / cost function` to minimise the prediction error. In this case, the gradient of SSE is a partial derivative of SSE w.r.t m and partial derivative of SSE w.r.t b.\n",
    "  - In the first step, we initialize m and b with random values. For the subsequent iterative process, m and b values are updated using step 3. In this step, we compute the partial derivative of SSE w.r.t m and partial derivative w.r.t b using the above equation for each data point in X. Finally, calculate the sum of all partial derivatives f w.r.t m and all partial derivatives f w.r.t b. In other words, `we compute the gradient of SSE for the data X`.\n",
    "- 3. Update coefficients in the direction of optimal m and b\n",
    "  - We can update the coefficients m and b using the gradient calculated from the above equations\n",
    "  - ![](readme_images/SSE.png)\n",
    "  - ![](readme_images/SSE_Derivatives_w.r.t_x&_y.png)\n",
    "  - ![](readme_images/SSE_Gradient.png)\n",
    "- 4. Use new m and b for prediction\n",
    "  - We use the data X with new m and b, computed in the above step, to draw the line that fit the data. \n",
    "  - ![](readme_images/Update_Weights_Using_Gradient_Derivatives.png)\n",
    "  - We calculate the SSE of each data point in X to find out the total SSE, which is a sum of squared errors of data \n",
    "  points, divided of 2. The total SSE indicates the error rate of the model prediction.\n",
    "  - ![](readme_images/Total_SSE.png)\n",
    "- 5. Repeat steps 2, 3 and 4\n",
    "  - ![](readme_images/Gradient_Plot.png)\n",
    "  - ![](readme_images/Optimal_Coefficients.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Observation`\n",
    "  - `Derivatives` gives the `rate of the change / slope / direction information of the function`.\n",
    "  - Here, function we are choosing is `Cost function - that checks difference between actuals & predictions`\n",
    "    - x = Regresion weights\n",
    "    - y = function(x) = SSE output\n",
    "  - Based on initial random Regression weights(x). We check SSE, if it is high means, derivatives [rate of the change / slope of the function(SSE)] is high too. But we want to reduce it.\n",
    "    - Because SSE[High] - Means high prediction error, poor predictions\n",
    "    - SSE[High] - Means low prediction error, good predictions\n",
    "  - So, we use this present rate of change information(i.e., derivatives) for getting better weights(i.e., to get less SSE) again using above computation formula.\n",
    "  - Again we check the SSE now, if derivatives [rate of the change / slope / direction of the function] is still high, we again we use latest rate of change information(i.e., derivatives) into getting better weights computation. We do this untill we get less SSE value.\n",
    "  - **`In short, derivatives[high dimension - Gradient] gives us better idea, about slope / how much the rate of change / direction of the cost function is. If it is high[weights are bad], if it is less[weights are good]. This is how derivatives / gradient used in getting better weights for Regression`**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
